{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../Tools')\n",
    "import frank_lab\n",
    "sys.path.append('../../../NeuroHMM/helpers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize': (12, 6),'lines.linewidth': 2, 'font.size': 18, \n",
    "            'axes.labelsize': 16, 'legend.fontsize': 12, 'ytick.labelsize': 12, 'xtick.labelsize': 12 })\n",
    "sns.set_style('white')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "import scipy.io\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fileroot = '/Users/ckemere/Data/Bon';\n",
    "fileroot = '../../../FrankLabData/Bon'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cellinfo.mat\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../../FrankLabData/Bon/cellinfo.mat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-395cf6532f7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcellinfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfrank_lab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_cellinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/ckemere/Development/Python/PrivatePython/Caleb/Tools/frank_lab.py\u001b[0m in \u001b[0;36mload_cellinfo\u001b[0;34m(fileroot)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m     mat = scipy.io.loadmat(os.path.join(fileroot,filename), \n\u001b[0;32m--> 227\u001b[0;31m                            struct_as_record=False, squeeze_me=True)\n\u001b[0m\u001b[1;32m    228\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cellinfo'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0mcellinfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ckemere/anaconda/lib/python3.5/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36mloadmat\u001b[0;34m(file_name, mdict, appendmat, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m     \"\"\"\n\u001b[1;32m    133\u001b[0m     \u001b[0mvariable_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'variable_names'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     \u001b[0mMR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmat_reader_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m     \u001b[0mmatfile_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmdict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ckemere/anaconda/lib/python3.5/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36mmat_reader_factory\u001b[0;34m(file_name, appendmat, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m        \u001b[0mtype\u001b[0m \u001b[0mdetected\u001b[0m \u001b[0;32min\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \"\"\"\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0mbyte_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0mmjv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmnv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_matfile_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyte_stream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmjv\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ckemere/anaconda/lib/python3.5/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36m_open_file\u001b[0;34m(file_like, appendmat)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mappendmat\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile_like\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.mat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../../FrankLabData/Bon/cellinfo.mat'"
     ]
    }
   ],
   "source": [
    "cellinfo, raw = frank_lab.load_cellinfo(fileroot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tetinfo,raw = frank_lab.load_data(fileroot, datatype='tetinfo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cellinfo.query('area==\"CA1\" & numspikes > 0')[\"Day\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "task = frank_lab.load_task(fileroot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos = frank_lab.load_pos(fileroot,day=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spikes = frank_lab.load_spikes(fileroot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dk = 3; \n",
    "#dk = 9; \n",
    "ep_list = list(task.query('Day==@dk & type==\"run\"')[\"Epoch\"]); \n",
    "print(ep_list); \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#placecellspikes = pd.merge(cellinfo.query('area==\"CA1\" & type==\"principal\" & numspikes > 0'), \n",
    "placecellspikes = pd.merge(cellinfo, \n",
    "                     spikes, \n",
    "                     on=['Day','Epoch', 'Tetrode', 'Cell'], # doing a join using these two columns to find matches\n",
    "                            # note that the result will be a cellinfo with all the task columns\n",
    "                     how='inner', # inner uses the intersection of keys from both frames\n",
    "                     suffixes=('','_')) # a tuple describing how\n",
    "\n",
    "ep = ep_list[0]\n",
    "num_cells = placecellspikes.query('Day==@dk & Epoch==@ep').shape[0]\n",
    "\n",
    "for ep in ep_list:\n",
    "    print(placecellspikes.query('Day==@dk & Epoch==@ep').shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for ep in ep_list:\n",
    "    print(pos.query('Day==@dk & Epoch==@ep').time.values[0][0], \n",
    "         pos.query('Day==@dk & Epoch==@ep').time.values[0][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Based on answer here: http://stackoverflow.com/questions/2566412/find-nearest-value-in-numpy-array\n",
    "def find_nearest(array,values):\n",
    "    right_idxs = np.searchsorted(array, values, side=\"left\")\n",
    "    left_idxs = np.where(right_idxs > 0, right_idxs-1, right_idxs)\n",
    "    right_idxs = np.where(right_idxs == len(array), len(array)-1, right_idxs)\n",
    "    closest_idx = np.where(np.abs(values - array[right_idxs]) < np.abs(values - array[left_idxs]),\n",
    "                          right_idxs, left_idxs)\n",
    "    return closest_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(1, figsize=(6, 4))\n",
    "plt.axes(aspect=1)\n",
    "\n",
    "for ep in ep_list:\n",
    "    postimes = pos.query('Day==@dk & Epoch==@ep').time.item() # first match (should be only one)\n",
    "    posx = pos.query('Day==@dk & Epoch==@ep').x.item() # first match (should be only one)\n",
    "    posy = pos.query('Day==@dk & Epoch==@ep').y.item() # first match (should be only one)\n",
    "    posvel = pos.query('Day==@dk & Epoch==@ep').vel.item() # first match (should be only one)\n",
    "    plt.plot(posx[posvel > 3], posy[posvel > 3],'k.')\n",
    "    for rowidx, spkitem in placecellspikes.query('Day==@dk & Epoch==@ep').iterrows() :\n",
    "        spikeposidx = find_nearest(postimes, spkitem.spiketimes)\n",
    "        #idx = np.searchsorted(postimes, all_spikes, side=\"left\")\n",
    "        #print(moving_idx.shape, idx.shape, posvel.shape)\n",
    "        spikeposidx_moving = spikeposidx[posvel[spikeposidx] > 3]\n",
    "        plt.plot(posx[spikeposidx_moving], posy[spikeposidx_moving],'.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "TetrodeList = list(set(tetinfo.query(\"Day==@dk & area=='CA1' & representative==1\").Tetrode.tolist()))\n",
    "print(TetrodeList)\n",
    "\n",
    "eeg_data = [[frank_lab.load_data(fileroot, day=dk+1, epoch=ep+1, tetrode=t+1, version='new').data \n",
    "             for t in TetrodeList] for ep in ep_list]\n",
    "\n",
    "FS = []\n",
    "StartTime = []\n",
    "TimeAxis = []\n",
    "for ep in ep_list:\n",
    "    eeg = frank_lab.load_data(fileroot, day=dk+1, epoch=ep+1, tetrode=TetrodeList[0]+1, version='new')\n",
    "    FS.append(eeg.samprate)\n",
    "    StartTime.append(eeg.starttime)\n",
    "    TimeAxis.append(eeg.starttime + np.array(range(len(eeg.data))) / eeg.samprate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Maggie defines ripples by doing:\n",
    "#  (1) filter 150-250\n",
    "#  (2) hilbert envelope\n",
    "#  (3) smooth with Gaussian (4 ms SD)\n",
    "#  (4) 3.5SD above the mean for 15 ms\n",
    "#  (5) full ripple defined as window back to mean\n",
    "\n",
    "import ripples\n",
    "SWR_bounds_idx = []\n",
    "SmoothedRippleEnvelope = []\n",
    "for epidx, ep in enumerate(ep_list): # loop over epochs\n",
    "    SWR_bounds_idx.append([])\n",
    "    for d in eeg_data[epidx]: # loop over electrodes\n",
    "        rip_idx, ripple_maxes, ripple_inner_bounds, ripple_data, ripple_envelope, smoothed_envelope = \\\n",
    "            ripples.detect(d, FS=FS[epidx], ThresholdSigma=3.5, LengthCriteria=0.015)\n",
    "        print(rip_idx.shape)\n",
    "        SWR_bounds_idx[epidx].append(np.array(rip_idx))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(1, figsize=(6, 4))\n",
    "\n",
    "for idx,riplist in enumerate(SWR_bounds_idx):\n",
    "    ep = ep_list[idx]\n",
    "    postimes = pos.query('Day==@dk & Epoch==@ep').time.item() # first match (should be only one)\n",
    "    posx = pos.query('Day==@dk & Epoch==@ep').x.item() # first match (should be only one)\n",
    "    posy = pos.query('Day==@dk & Epoch==@ep').y.item() # first match (should be only one)\n",
    "    posvel = pos.query('Day==@dk & Epoch==@ep').vel.item() # first match (should be only one)\n",
    "    ripposidx = find_nearest(postimes, SWR_bounds_idx[idx][0][:,0]/FS[0] + StartTime[idx])\n",
    "    plt.plot(postimes,posvel)\n",
    "    plt.plot(postimes[ripposidx],posvel[ripposidx],'o')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from hmmlearn import hmm # see https://github.com/ckemere/hmmlearn\n",
    "#import klabtools as klab\n",
    "#import seqtools as sq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from binning import bin_spikes\n",
    "## bin ALL spikes\n",
    "ds = 0.125 # bin spikes into 125 ms bins (theta-cycle inspired)\n",
    "\n",
    "all_spikes = []\n",
    "bin_centers = []\n",
    "for ep in ep_list:\n",
    "    [bs, bb] = bin_spikes(\n",
    "        [spkitem.spiketimes  for rowidx, spkitem in placecellspikes.query('Day==@dk & Epoch==@ep').iterrows()],\n",
    "        ds=ds, fs = 1)\n",
    "    all_spikes.append(bs)\n",
    "    bin_centers.append(bb[:-1] + ds/2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Next, find speeds for each bin\n",
    "bin_vels = []\n",
    "for epidx,ep in enumerate(ep_list):\n",
    "    postimes = pos.query('Day==@dk & Epoch==@ep').time.item() # first match (should be only one)\n",
    "    posvel = pos.query('Day==@dk & Epoch==@ep').vel.item() # first match (should be only one)\n",
    "    binposidx = find_nearest(postimes, bin_centers[epidx])\n",
    "    bin_vels.append(posvel[binposidx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(1, figsize=(6, 4))\n",
    "plt.plot(bin_vels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract sequences of spikes during which rat is moving faster than a threshold speed\n",
    "speed_threshold = 3\n",
    "moving_chunks = []\n",
    "moving_bounds_idx = []\n",
    "moving_bounds_t = []\n",
    "for epidx,ep in enumerate(ep_list):\n",
    "    moving_chunks.append([])\n",
    "    moving_bounds_idx.append([])\n",
    "    moving_bounds_t.append([])\n",
    "    for s in np.ma.clump_masked(np.ma.masked_greater(bin_vels[epidx], speed_threshold)):\n",
    "        moving_chunks[-1].append(all_spikes[epidx][s,:])\n",
    "        moving_bounds_idx[-1].append([s.start, s.stop])\n",
    "        moving_bounds_t[-1].append(bin_centers[epidx][0] + [ds * s.start, ds * s.stop])\n",
    "\n",
    "    moving_bounds_idx[-1] = np.array(moving_bounds_idx[-1]) # convert list to np.array\n",
    "    moving_bounds_t[-1] = np.array(moving_bounds_t[-1]) # convert list to np.array\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from binning import windowed_bin_spikes\n",
    "## bin SWR spikes\n",
    "ds_swr = 0.01 # bin spikes into 10 ms bins somewhat standard\n",
    "electrode = 0\n",
    "\n",
    "swr_binned_spikes = []\n",
    "swr_bin_centers = []\n",
    "for epidx,ep in enumerate(ep_list):\n",
    "    swr_bnds = SWR_bounds_idx[epidx][electrode]\n",
    "    swr_bnds = StartTime[epidx] + swr_bnds/FS[epidx]\n",
    "    [bs, bb] = windowed_bin_spikes(\n",
    "        [spkitem.spiketimes  for rowidx, spkitem in placecellspikes.query('Day==@dk & Epoch==@ep').iterrows()],\n",
    "        swr_bnds, ds=ds_swr)\n",
    "    swr_binned_spikes.append(bs)\n",
    "    bc = [ev[:-1] + ds_swr/2 for ev in bb]\n",
    "    bin_centers.append(bc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For training, use the moving sequences\n",
    "chunked_data = moving_chunks.copy()\n",
    "chunked_bounds_idx = moving_bounds_idx.copy()\n",
    "chunked_bounds_t = moving_bounds_t.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import seqtools as sq\n",
    "\n",
    "print(type(chunked_bounds_t[0]))\n",
    "print(chunked_bounds_t[0].shape)\n",
    "\n",
    "# Test model for novel track\n",
    "mapdata = type('Map',(object,),dict(data=chunked_data[0], bin_width=ds, \n",
    "                                    boundaries=chunked_bounds_t[0], boundaries_fs=1))\n",
    "## stack data for hmmlearn:\n",
    "seq_stk_bvr = sq.data_stack(mapdata, verbose=True)\n",
    "\n",
    "## split data into train, test, and validation sets:\n",
    "tr_b,vl_b,ts_b = sq.data_split(seq_stk_bvr, tr=60, vl=20, ts=20, randomseed = 0, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Smax = 60\n",
    "S = np.arange(start=5,step=5,stop=Smax+1)\n",
    "\n",
    "tr_ll = []\n",
    "vl_ll = []\n",
    "ts_ll = []\n",
    "\n",
    "for num_states in S:\n",
    "    print('Training and evaluating {}-state hmm'.format(num_states))\n",
    "    sys.stdout.flush()\n",
    "    myhmm = sq.hmm_train(tr_b, num_states=num_states, n_iter=30, verbose=False)\n",
    "    tr_ll.append( (np.array(list(sq.hmm_eval(myhmm, tr_b)))/tr_b.sequence_lengths ).mean())\n",
    "    vl_ll.append( (np.array(list(sq.hmm_eval(myhmm, vl_b)))/vl_b.sequence_lengths ).mean())\n",
    "    ts_ll.append( (np.array(list(sq.hmm_eval(myhmm, ts_b)))/ts_b.sequence_lengths ).mean())\n",
    "\n",
    "print('Done!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "fig = plt.figure(1, figsize=(5, 3))\n",
    "ax = fig.add_subplot(111)\n",
    "    \n",
    "ax.plot(S, tr_ll, lw=2, linestyle='dotted', label='train', color='k')\n",
    "ax.plot(S, ts_ll, lw=2, linestyle='dashed', label='test', color='k')\n",
    "ax.plot(S, vl_ll, lw=2, linestyle='solid', label='validation', color='mediumseagreen')\n",
    "ax.legend(loc=2)\n",
    "ax.set_xlabel('number of states')\n",
    "ax.set_ylabel('normalized log likelihood')\n",
    "\n",
    "ax.set_xlim([5, S[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check whether familiar model is good in the same size range\n",
    "\n",
    "mapdata_fam = type('Map',(object,),dict(data=chunked_data[2], bin_width=ds, \n",
    "                                    boundaries=chunked_bounds_t[2], boundaries_fs=1))\n",
    "## stack data for hmmlearn:\n",
    "seq_stk_bvr_fam = sq.data_stack(mapdata_fam, verbose=True)\n",
    "\n",
    "## split data into train, test, and validation sets:\n",
    "tr_b_fam,vl_b_fam,ts_b_fam = sq.data_split(seq_stk_bvr, tr=60, vl=20, ts=20, randomseed = 0, verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "S = np.arange(start=5,step=5,stop=Smax+1)\n",
    "\n",
    "tr_ll_fam = []\n",
    "vl_ll_fam = []\n",
    "ts_ll_fam = []\n",
    "\n",
    "for num_states in S:\n",
    "    print('Training and evaluating {}-state hmm'.format(num_states))\n",
    "    sys.stdout.flush()\n",
    "    myhmm_fam = sq.hmm_train(tr_b_fam, num_states=num_states, n_iter=30, verbose=False)\n",
    "    tr_ll_fam.append( (np.array(list(sq.hmm_eval(myhmm_fam, tr_b_fam)))/tr_b_fam.sequence_lengths ).mean())\n",
    "    vl_ll_fam.append( (np.array(list(sq.hmm_eval(myhmm_fam, vl_b_fam)))/vl_b_fam.sequence_lengths ).mean())\n",
    "    ts_ll_fam.append( (np.array(list(sq.hmm_eval(myhmm_fam, ts_b_fam)))/ts_b_fam.sequence_lengths ).mean())\n",
    "\n",
    "print('Done!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "fig = plt.figure(1, figsize=(5, 3))\n",
    "ax = fig.add_subplot(111)\n",
    "    \n",
    "ax.plot(S, tr_ll_fam, lw=2, linestyle='dotted', label='train', color='k')\n",
    "ax.plot(S, ts_ll_fam, lw=2, linestyle='dashed', label='test', color='k')\n",
    "ax.plot(S, vl_ll_fam, lw=2, linestyle='solid', label='validation', color='mediumseagreen')\n",
    "ax.legend(loc=2)\n",
    "ax.set_xlabel('number of states')\n",
    "ax.set_ylabel('normalized log likelihood')\n",
    "\n",
    "ax.set_xlim([5, S[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hmm = []\n",
    "num_states = 30\n",
    "stacked_data = []\n",
    "for epidx, ep in enumerate(ep_list):\n",
    "    mapdata = type('Map',(object,),dict(data=chunked_data[epidx], bin_width=ds, \n",
    "                                        boundaries=chunked_bounds_t[epidx], boundaries_fs=1))\n",
    "\n",
    "    ## stack data for hmmlearn:\n",
    "    stacked_data.append(sq.data_stack(mapdata, verbose=True))\n",
    "    ## split data into train, test, and validation sets:\n",
    "    tr,vl,ts = sq.data_split(stacked_data[-1], tr=60, vl=20, ts=20, randomseed = 0, verbose=False)\n",
    "    hmm.append(sq.hmm_train(tr, num_states=num_states, n_iter=30, verbose=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(1, figsize=(6, 4))\n",
    "sns.distplot(stacked_data[0].sequence_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_replay_score_from_hmmlearn( hmmlearnModel, pth, obs ):\n",
    "    # pth is a list of state indices, obs is a list (seq len) of lists (num cells) of nparray(rate)\n",
    "    #P = np.exp(ymModel.dense_transition_matrix())\n",
    "    logP = np.log(hmmlearnModel.transmat_)\n",
    "    logPseq = 0\n",
    "    logPctx = 0\n",
    "    for ii in np.arange( 0, len(pth)-1 ):\n",
    "        # add transition probability to sequence score:\n",
    "        logPseq += logP[pth[ii],pth[ii+1]]\n",
    "        # add memoryless observation likelihood to contextual score:\n",
    "        logPctx += hmmlearnModel.score(obs[ii,:].reshape(1,-1)) # memoryless map probability per symbol FFFB! What about prior state probs?\n",
    "        #logPctx += np.log(hmmlearnModel.predict_proba(obs[ii,:])[0][hmmlearnModel.decode(obs[ii,:])[1][0]]) # memoryless map probability per symbol FFFB! What about prior state probs?model.predict_proba(obs[ii-1,:])[0][model.decode(obs[ii-1,:])[1][0]]\n",
    "        #print(logPctx)\n",
    "    logPctx += hmmlearnModel.score(obs[-1,:].reshape(1,-1)) # memoryless map probability per symbol\n",
    "    #logPctx += np.log(hmmlearnModel.predict_proba(obs[-1,:])[0][hmmlearnModel.decode(obs[-1,:])[1][0]]) # memoryless map probability per symbol FFFB! What about prior state probs?model.predict_proba(obs[ii-1,:])[0][model.decode(obs[ii-1,:])[1][0]]\n",
    "    \n",
    "    return logPseq/len(pth), logPctx/len(pth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def advance_states_one(pp, A):\n",
    "    return np.dot(pp, A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def advance_states_one(pp, A):\n",
    "    return np.dot(pp, A)\n",
    "\n",
    "def KLscore(distr_matU, distr_matV):\n",
    "    from scipy.stats import entropy as KLD\n",
    "    num_bins = distr_matU.shape[0]\n",
    "\n",
    "    KLarray = np.zeros(num_bins)\n",
    "    \n",
    "    for ii in np.arange(1,num_bins):\n",
    "        KLarray[ii-1] = KLD(distr_matU[ii,:],distr_matV[ii,:])\n",
    "        \n",
    "    KLscore = []\n",
    "    for ii, KLi in enumerate(KLarray):\n",
    "        if KLi != 0: #FFB! HOW SHOULD I TREAT THESE CASES PROPERLY? probably by enforcing a prior on my pmfs\n",
    "            KLscore.append(1 / KLi)\n",
    "        else:\n",
    "            KLscore.append(1e5)\n",
    "    \n",
    "    return np.sum(np.log(KLscore)/len(KLscore))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compute sequence scores on all candidate events, and compare to trajectory shuffle:\n",
    "from random import shuffle\n",
    "\n",
    "scores = []\n",
    "uniq_len = []\n",
    "seq_len = []\n",
    "for model_idx in range(len(ep_list)):\n",
    "    scores.append([])\n",
    "    uniq_len.append([])\n",
    "    seq_len.append([])\n",
    "    \n",
    "    m = hmm[model_idx].n_components\n",
    "    Pi = hmm[model_idx].startprob_.copy()\n",
    "    Pi = np.reshape(Pi,(1,m))\n",
    "    A = hmm[model_idx].transmat_.copy()\n",
    "\n",
    "    for ts_idx in range(len(ep_list)):\n",
    "        data_to_test = stacked_data[ts_idx]\n",
    "\n",
    "        myStackedDataSeq = data_to_test.data.copy()\n",
    "        myStackedSeqLengths = data_to_test.sequence_lengths.copy()\n",
    "\n",
    "        scores[model_idx].append({})\n",
    "        scores[model_idx][ts_idx]['seq'] = np.zeros(len(myStackedSeqLengths))\n",
    "        scores[model_idx][ts_idx]['seq_KL'] = np.zeros(len(myStackedSeqLengths))\n",
    "        scores[model_idx][ts_idx]['ctx'] = np.zeros(len(myStackedSeqLengths))\n",
    "        scores[model_idx][ts_idx]['shfl_seq'] = np.zeros(len(myStackedSeqLengths))\n",
    "        scores[model_idx][ts_idx]['shfl_seq_KL'] = np.zeros(len(myStackedSeqLengths))\n",
    "        scores[model_idx][ts_idx]['shfl_ctx'] = np.zeros(len(myStackedSeqLengths))\n",
    "\n",
    "        uniq_len[model_idx].append([])\n",
    "\n",
    "        seqlimits = np.cumsum(np.array([0] + list(myStackedSeqLengths)))\n",
    "        #for ee in np.arange(0,len(myStackedSeqLengths)):\n",
    "        for ee in range(len(chunked_data[ts_idx])):\n",
    "            obs = chunked_data[ts_idx][ee]\n",
    "            lp, pth = hmm[model_idx].decode(obs,algorithm='viterbi') # not sure that we should use viterbi for scoring?\n",
    "            \n",
    "            ll, pp = hmm[model_idx].score_samples(obs) # memoryless posterior\n",
    "            ppp = advance_states_one(np.vstack([Pi,pp[:obs.shape[0]-1,:]]), A)\n",
    "            \n",
    "            #obs = myStackedDataSeq[seqlimits[ee]:seqlimits[ee+1],:]\n",
    "            #lp, pth = hmm[model_idx].decode(obs,algorithm='viterbi')\n",
    "            trj_shfl_idx = np.arange(0,len(pth))\n",
    "            shuffle(trj_shfl_idx)\n",
    "            pth_trj_shfl = [pth[i] for i in trj_shfl_idx]\n",
    "            obs_trj_shfl = np.array([obs[i] for i in trj_shfl_idx])\n",
    "            \n",
    "            ll_shfl, pp_shfl = hmm[model_idx].score_samples(obs_trj_shfl) # memoryless posterior\n",
    "#             ppp_shfl = advance_states_one(np.vstack([Pi,pp_shfl[:obs.shape[0]-1,:]]), A)\n",
    "            ppp_shfl = advance_states_one(np.vstack([Pi,pp_shfl[1:obs.shape[0]-1,:]]), A)\n",
    "\n",
    "            scores[model_idx][ts_idx]['seq'][ee], scores[model_idx][ts_idx]['ctx'][ee] = \\\n",
    "                get_replay_score_from_hmmlearn(hmm[model_idx], pth, obs)\n",
    "            scores[model_idx][ts_idx]['shfl_seq'][ee], scores[model_idx][ts_idx]['shfl_ctx'][ee] = \\\n",
    "                get_replay_score_from_hmmlearn(hmm[model_idx], pth_trj_shfl, obs_trj_shfl)\n",
    "            scores[model_idx][ts_idx]['seq_KL'][ee] = KLscore(pp,ppp)\n",
    "            scores[model_idx][ts_idx]['shfl_seq_KL'][ee] = KLscore(pp_shfl,ppp_shfl)\n",
    "            #print(ee)\n",
    "\n",
    "            uniq_len[model_idx][ts_idx].append(len(set(pth))) # number of unique states visited in path\n",
    "\n",
    "        seq_len[model_idx].append(myStackedSeqLengths) # sequence lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "f, axs = plt.subplots(3,3, figsize=(10, 10), sharex=True, sharey=True)\n",
    "for model_idx in range(len(ep_list)):\n",
    "    for ts_idx in range(len(ep_list)):\n",
    "        ax = axs[model_idx,ts_idx]\n",
    "        ax.scatter(np.exp(scores[model_idx][ts_idx]['seq'])*np.array(uniq_len[model_idx][ts_idx]), \n",
    "                   scores[model_idx][ts_idx]['ctx'], \n",
    "                   s=80, c='mediumseagreen', alpha=0.3, marker='o',facecolors='none', \n",
    "                   edgecolors='gray',linewidth=1, label='candidate seq')\n",
    "        ax.scatter(np.exp(scores[model_idx][ts_idx]['seq_KL'])*np.array(uniq_len[model_idx][ts_idx]), \n",
    "                   scores[model_idx][ts_idx]['ctx'], \n",
    "                   s=80, c='skyblue', alpha=0.3, marker='o',facecolors='none', \n",
    "                   edgecolors='gray',linewidth=1, label='candidate seq, KL')\n",
    "        ax.scatter(np.exp(scores[model_idx][ts_idx]['shfl_seq'])*np.array(uniq_len[model_idx][ts_idx]), \n",
    "                   scores[model_idx][ts_idx]['shfl_ctx'], \n",
    "                   c='k', s=80, marker='.',facecolors='none', edgecolors='none',\n",
    "                   linewidth=1, label='trajectory shuffled')\n",
    "        ax.scatter(np.exp(scores[model_idx][ts_idx]['shfl_seq_KL'])*np.array(uniq_len[model_idx][ts_idx]), \n",
    "                   scores[model_idx][ts_idx]['shfl_ctx'], \n",
    "                   c='orange', s=80, marker='.',facecolors='none', edgecolors='none',\n",
    "                   linewidth=1, label='trajectory shuffled, KL')\n",
    "        ax.axvline(x=1, linewidth=1, color = 'gray', linestyle='dotted')\n",
    "        ax.set_xlabel('$\\exp(q_{seq})$')\n",
    "        ax.set_ylabel('$q_{ctx}$')\n",
    "        ax.legend(loc='lower right', fancybox=True, framealpha=0.5, frameon=True)\n",
    "        ax.set_xlim([-10,50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f, axs = plt.subplots(1,3, figsize=(15, 5), sharex=True, sharey=True)\n",
    "cols = ['deepskyblue','mediumseagreen','mistyrose']\n",
    "markers = ['o','v','s']\n",
    "for ts_idx in range(len(ep_list)):\n",
    "    ax = axs[ts_idx]\n",
    "    for model_idx in range(len(ep_list)):\n",
    "        ax.scatter(scores[ts_idx][ts_idx]['ctx'], \n",
    "                   scores[model_idx][ts_idx]['ctx'], \n",
    "                   s=10, c=cols[model_idx], alpha=0.9, marker=markers[model_idx],facecolors='none', \n",
    "                   edgecolors='gray',linewidth=1, label='ep$_{{{0}}}$ model'.format(model_idx))\n",
    "        ax.plot([-60,0],[-60,0], linewidth=1, color = 'gray', linestyle='dotted')\n",
    "        ax.legend(loc='upper left', fancybox=True, framealpha=0.5, frameon=True)\n",
    "    ax.set_xlabel('$q_{ctx}$, model = data')\n",
    "    ax.set_ylabel('$q_{ctx}$, alternate model')\n",
    "    ax.set_title('Epoch {}'.format(ts_idx))\n",
    "    ax.set_ylim([-25, 0])\n",
    "    ax.set_xlim(ax.get_ylim())\n",
    "    ax.set_aspect('equal', 'datalim')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f, axs = plt.subplots(1, figsize=(5, 5), sharex=True, sharey=True)\n",
    "axs.plot(uniq_len[model_idx][ts_idx], scores[model_idx][ts_idx]['ctx'],'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate state fields and place fields for a particular epoch\n",
    "\n",
    "epidx = 2\n",
    "ep = ep_list[epidx]\n",
    "\n",
    "# Pull out position data for selected epoch \n",
    "postimes = pos.query('Day==@dk & Epoch==@ep').time.item() # first match (should be only one)\n",
    "posvel = pos.query('Day==@dk & Epoch==@ep').vel.item() # first match (should be only one)\n",
    "posx = pos.query('Day==@dk & Epoch==@ep').x.item() # first match (should be only one)\n",
    "posy = pos.query('Day==@dk & Epoch==@ep').y.item() # first match (should be only one)\n",
    "\n",
    "# get position on the same timescale as the observations\n",
    "interpposx = np.interp(bin_centers[epidx], postimes, posx)\n",
    "interpposy = np.interp(bin_centers[epidx], postimes, posy)\n",
    "\n",
    "f, axs = plt.subplots(1, figsize=(5, 5))\n",
    "plt.plot(interpposx,interpposy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## 2D place field approximation\n",
    "nbins = 100\n",
    "xy00 = 0\n",
    "xyLL = 250\n",
    "bins = np.linspace(xy00, xyLL, nbins)\n",
    "\n",
    "# Compute animal occupancy per spatial bin by adding up the number of time\n",
    "#   bins he spends in each bin\n",
    "hbins = np.linspace(xy00, xyLL, nbins+1) # histogram wants bin edges\n",
    "H, _, _ = np.histogram2d(posx, posy, bins=hbins)\n",
    "NormalizedOccupancy= H.copy()\n",
    "NormalizedOccupancy[H==0] = np.nan\n",
    "\n",
    "# Learn state / position mapping and place / position mapping by\n",
    "#  iterating through each sequence and adding up all the state likelihoods/spikes\n",
    "#  with the corresponding positions in each time bin\n",
    "\n",
    "num_sequences = len(chunked_data[epidx])\n",
    "num_states = hmm[epidx].n_components\n",
    "place_fields = np.zeros((num_cells, nbins, nbins)) # stack of 2D place fields\n",
    "state_pos2D = np.zeros((num_states, nbins, nbins)) # stack of 2D place fields\n",
    "\n",
    "for seq_id in range(num_sequences):\n",
    "    obs = chunked_data[epidx][seq_id]\n",
    "    ll, pp = hmm[epidx].score_samples(obs)\n",
    "    if num_sequences == 1:\n",
    "        xx = interpposx\n",
    "        yy = interpposy\n",
    "    else:\n",
    "        xx = interpposx[chunked_bounds_idx[epidx][seq_id][0]:chunked_bounds_idx[epidx][seq_id][1] +1]\n",
    "        yy = interpposy[chunked_bounds_idx[epidx][seq_id][0]:chunked_bounds_idx[epidx][seq_id][1] +1]\n",
    "    dxx = np.digitize(xx, bins) - 1 # spatial bin numbers\n",
    "    dyy = np.digitize(yy, bins) - 1 # spatial bin numbers\n",
    "    for ii, ppii in enumerate(pp):\n",
    "        state_pos2D[:,dxx[ii],dyy[ii]] += np.transpose(ppii)\n",
    "        place_fields[:, dxx[ii], dyy[ii]] += np.transpose(obs[ii,:])\n",
    "\n",
    "# place_fields[place_fields==0] = np.nan\n",
    "# state_pos2D[state_pos2D==0] = np.nan\n",
    "\n",
    "# normalize place fields\n",
    "for ii, stateposfield in enumerate(state_pos2D):\n",
    "        state_pos2D[ii,:,:] = stateposfield / NormalizedOccupancy\n",
    "for ii, pf in enumerate(place_fields):\n",
    "        place_fields[ii,:,:] = pf / NormalizedOccupancy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Aside - plot observation matrix for model\n",
    "fig = plt.figure(1, figsize=(9, 7))\n",
    "ax = fig.add_subplot(111)\n",
    "im = ax.matshow(hmm[epidx].means_,interpolation='none', cmap='RdPu',  )\n",
    "fig.colorbar(im)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Want to blank out space where the rat never goes\n",
    "SpaceOverlay = H.copy() # Opposite of NormalizedOccupancy\n",
    "SpaceOverlay[H>0] = np.nan \n",
    "\n",
    "import matplotlib.colors as pltcolors\n",
    "NiceBG = pltcolors.ListedColormap(['0.6'])\n",
    "\n",
    "sns.set_style('dark')\n",
    "fig = plt.figure(1, figsize=(5, 5))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "state = 2\n",
    "ax.matshow(SpaceOverlay,interpolation='none', cmap=NiceBG  )\n",
    "# pf= ax.matshow(place_fields[state,::], cmap='RdPu')\n",
    "pf = ax.matshow(state_pos2D[state,::], cmap='RdPu')\n",
    "fig.colorbar(pf, ax=ax)\n",
    "ax.set_xticklabels([])\n",
    "ax.set_yticklabels([])\n",
    "ax.set_title('state ' + str(state))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(6, 5, figsize=(17, 17), sharex=True, sharey=True)\n",
    "axes = [item for sublist in axes for item in sublist]\n",
    "\n",
    "\n",
    "#SpaceOverlay[H>0] = 1\n",
    "for state, ax in enumerate(axes):\n",
    "    ax.matshow(SpaceOverlay,interpolation='none', cmap=NiceBG  )\n",
    "    # pf= ax.matshow(place_fields[state,::], cmap='RdPu')\n",
    "    pf = ax.matshow(state_pos2D[state,::], cmap='RdPu')\n",
    "    fig.colorbar(pf, ax=ax)\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_title('state ' + str(state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(swr_binned_spikes[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compute sequence scores on all candidate swr events, and compare to trajectory shuffle:\n",
    "from random import shuffle\n",
    "\n",
    "swr_scores = []\n",
    "swr_uniq_len = []\n",
    "#swr_seq_len = []\n",
    "for model_idx in range(len(ep_list)):\n",
    "    swr_scores.append([])\n",
    "    swr_uniq_len.append([])\n",
    "    #swr_seq_len.append([])\n",
    "    for ts_idx in range(len(ep_list)):\n",
    "        swr_scores[model_idx].append({})\n",
    "        swr_scores[model_idx][ts_idx]['seq'] = np.zeros(len(swr_binned_spikes[ts_idx]))\n",
    "        swr_scores[model_idx][ts_idx]['ctx'] = np.zeros(len(swr_binned_spikes[ts_idx]))\n",
    "        swr_scores[model_idx][ts_idx]['shfl_seq'] = np.zeros(len(swr_binned_spikes[ts_idx]))\n",
    "        swr_scores[model_idx][ts_idx]['shfl_ctx'] = np.zeros(len(swr_binned_spikes[ts_idx]))\n",
    "\n",
    "        swr_uniq_len[model_idx].append([])\n",
    "\n",
    "        for ee in range(len(swr_binned_spikes[ts_idx])):\n",
    "            obs = swr_binned_spikes[ts_idx][ee]\n",
    "            lp, pth = hmm[model_idx].decode(obs,algorithm='viterbi')\n",
    "            \n",
    "            trj_shfl_idx = np.arange(0,len(pth))\n",
    "            shuffle(trj_shfl_idx)\n",
    "            pth_trj_shfl = [pth[i] for i in trj_shfl_idx]\n",
    "            obs_trj_shfl = np.array([obs[i] for i in trj_shfl_idx])\n",
    "\n",
    "            swr_scores[model_idx][ts_idx]['seq'][ee], swr_scores[model_idx][ts_idx]['ctx'][ee] = \\\n",
    "                get_replay_score_from_hmmlearn(hmm[model_idx], pth, obs)\n",
    "            swr_scores[model_idx][ts_idx]['shfl_seq'][ee], swr_scores[model_idx][ts_idx]['shfl_ctx'][ee] = \\\n",
    "                get_replay_score_from_hmmlearn(hmm[model_idx], pth_trj_shfl, obs_trj_shfl)\n",
    "\n",
    "            swr_uniq_len[model_idx][ts_idx].append(len(set(pth))) # number of unique states visited in path\n",
    "\n",
    "        # swr_seq_len[model_idx].append(np.array([len(s) for s in swr_binned_spikes[ts_idx]])) # sequence lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "f, axs = plt.subplots(3,3, figsize=(10, 10), sharex=True, sharey=True)\n",
    "for model_idx in range(len(ep_list)):\n",
    "    for ts_idx in range(len(ep_list)):\n",
    "        ax = axs[model_idx,ts_idx]\n",
    "        ax.scatter(np.exp(swr_scores[model_idx][ts_idx]['seq'])*np.array(swr_uniq_len[model_idx][ts_idx]), \n",
    "                   swr_scores[model_idx][ts_idx]['ctx'], \n",
    "                   s=80, c='mediumseagreen', alpha=0.3, marker='o',facecolors='none', \n",
    "                   edgecolors='gray',linewidth=1, label='candidate sequences')\n",
    "        ax.scatter(np.exp(swr_scores[model_idx][ts_idx]['shfl_seq'])*np.array(swr_uniq_len[model_idx][ts_idx]), \n",
    "                   swr_scores[model_idx][ts_idx]['shfl_ctx'], \n",
    "                   c='k', s=80, marker='.',facecolors='none', edgecolors='none',\n",
    "                   linewidth=1, label='trajectory shuffled')\n",
    "        ax.axvline(x=1, linewidth=1, color = 'gray', linestyle='dotted')\n",
    "        ax.set_xlabel('$\\exp(q_{seq})$')\n",
    "        ax.set_ylabel('$q_{ctx}$')\n",
    "        ax.legend(loc='lower right', fancybox=True, framealpha=0.5, frameon=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f, axs = plt.subplots(1,3, figsize=(15, 5), sharex=True, sharey=True)\n",
    "cols = ['deepskyblue','mediumseagreen','mistyrose']\n",
    "markers = ['o','v','s']\n",
    "for ts_idx in range(len(ep_list)):\n",
    "    ax = axs[ts_idx]\n",
    "    for model_idx in range(len(ep_list)):\n",
    "        ax.scatter(swr_scores[ts_idx][ts_idx]['ctx'], \n",
    "                   swr_scores[model_idx][ts_idx]['ctx'], \n",
    "                   s=10, c=cols[model_idx], alpha=0.9, marker=markers[model_idx],facecolors='none', \n",
    "                   edgecolors='gray',linewidth=1, label='ep$_{{{0}}}$ model'.format(model_idx))\n",
    "        #ax.scatter(swr_scores[ts_idx][ts_idx]['shfl_ctx'], \n",
    "        #           swr_scores[model_idx][ts_idx]['shfl_ctx'], \n",
    "        #           c='k', s=80, marker='.',facecolors='none', edgecolors='none',\n",
    "        #           linewidth=1, label='trajectory shuffled')\n",
    "        ax.plot([-20,0],[-20,0], linewidth=1, color = 'gray', linestyle='dotted')\n",
    "        #ax.axvline(x=1, linewidth=1, color = 'gray', linestyle='dotted')\n",
    "        ax.legend(loc='upper left', fancybox=True, framealpha=0.5, frameon=True)\n",
    "    ax.set_xlabel('$q_{ctx}$, model = data')\n",
    "    ax.set_ylabel('$q_{ctx}$, alternate model')\n",
    "    ax.set_title('Epoch {}'.format(ts_idx))\n",
    "    ax.set_ylim([-15, 0])\n",
    "    ax.set_xlim(ax.get_ylim())\n",
    "    ax.set_aspect('equal', 'datalim')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
